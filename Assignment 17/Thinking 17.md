##### Thinking 1：假设输入是100\*100的灰度图像，现在你使用卷积对图像进行特征提取，有50个滤波器，每个卷积核是5*5大小，那么在隐藏层会有多少参数（需要考虑bias参数）

可训练参数 = （卷积核面积 + 1）* 卷积核数量 = （5*5+1） * 50 = 1300



##### Thinking 2：局部不变性和参数共享指的是什么？

CNN通过局部感受野，从以前的全连接更改为局部连接，使局部特征不随图像放大缩小而改变。同时也可以通过augmentation（平移、旋转、拉伸、翻转等）使算法学习更多的同张图片的不同状态下的特征，使算法辨识度更好。



CNN通过卷积核（滤波器）生成feature map，实现参数共享或权重共享，目的是为了减少学习的参数，提升学习效率以及泛化能力。



##### Thinking 3：为什么会用到batch normalization ?

BN是通过规范化的方式，把神经网络中每个神经元的输入值分布强行拉到均值为0方差为1的标准正态分布中，使非线性变换函数的输入值落入对其较为敏感的区域，避免梯度消失的问题，增加收敛速度，提升训练速度。同时，由于BN采用了mini-batch的方式，每个batch的均值和方差都会有一定不同，增加了网络在学习过程中带来的随机噪音，相当于Dropout带来的随机噪音一样，所以BN具有一定的正则化作用。



##### Thinking 4：使用dropout可以解决什么问题？

相当于机器学习中的正则化，减少神经网络的过拟合



##### Thinking 5：ResNet中的Residual Block解决了什么问题

CNNing的深度到达某种程度之后模型效果可能会不升反降，使模型发生退化，通过residual block可以把残差通过跳跃的方式前向传播至好几层之后的网络（跳跃连接），让神经网络学习之前网络的残差，提高准确率。

