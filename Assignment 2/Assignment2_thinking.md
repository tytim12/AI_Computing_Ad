### Lesson 2 Assignments

#### Thinking 1: 举一个你之前做过的预测例子
<table><tr><td bgcolor=#F5F6F7>
    回答：  
    </br>
    未曾在工作中做过预测的应用。但是曾经通过比较传统的统计模型（双样本T检验）的方式，判断公司针对社区做的活动，在活动前中后期对APP活跃度的影响是否为显著</td></tr></table>



#### Thinking 2：XGBoost, LightGBM, CatBoost是三种基于GBDT的实现，三者之间区别是怎样的  

<table><tr><td bgcolor=#F5F6F7>
    回答:  
    </br>
1. XGBoost：不停的增加分枝，每次增加分枝的时候就学习一个新函数来拟合上一次的残差。寻找分隔值时，通过分类和直方图的方法进行采样。 </br> 
2. LightGBM：效率更高，得到的结果准确度和精度与XGBoost不相上下。寻找分隔值时，结构上是通过GOSS进行采样</br>  
3. CatBoost: 在分类问题上的效率和准确度更高，在寻找分隔值的时候，是通过赋予分类变量指标进行独热编码 </br> 
4. 共同点：参数多，调参时间长  </br>
</td></tr></table>



#### Thinking 3: 你认为，NGBoost对之后的算法会有怎样的影响  

<table><tr><td bgcolor=#F5F6F7>回答：</br>1. NGB 是通过引入概率预测引入到梯度提升当中，解决现有梯度提升中无法解决的技术难题  </br>
2. 通过课上的图例展示，可以看出NGB对于数据分布的拟合能力十分强大  </br>
3. 而NGB（或者GBDT家族）虽然主要是为了解决了回归的问题（广义），但由于其它GBDT家族在某些区间的拟合能力比较差，在线性回归的问题上效率应该不高（虽然考虑到decision tree可以无限逼近所有连续函数）</br>  
4. 结合上述#2和#3，个人认为NGB在线性回归（特别是多元的）以及多分类问题上可能会有进一步的突破。</br>
5. 题外话：最近看到一篇文章，讲述贝叶斯神经网络对比一般的神经网络是否没用，因为贝叶斯神经网络学习到的bias和weights是概率分布，而不是确定的数值。从NGB的拟合能力考虑，未来可能在神经网络中也会逐渐接受概率形式的神经网络，通过这种方式（而不是正则化）泛化神经网络，降低过拟合风险。同时对稀疏的数据集或者缺失数据的处理会更好。</br></td></tr></table>



