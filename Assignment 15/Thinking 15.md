##### Thinking 1： 机器学习中的监督学习、非监督学习、强化学习有何区别

监督学习：带label，通过训练数据得到反馈，更新loss

非监督学习：无label，只有数据特征，没有输出反馈或奖励

强化学习：有正负反馈及奖励值，和监督学习的输出值相比是延迟反馈；和非监督学习相比强化学习有前后依赖关系



##### Thinking 2： 什么是策略网络，价值网络，有何区别

策略网络：给定输入，通过学习输出一个确定输出的网络，如：输入动作1得到状态1，输入动作2得到状态2。最终得到一个状态集合S，动作集A，给定状态和动作的奖励分布R，给定状态转移到下一状态的概率分布P，防止奖励过大的贴现因子Y，以及以上内容组成的最有策略π

价值网络：通过计算当前状态的累计分数的期望，给状态赋予一个分数值，奖励更多数值越大。

通过策略网络得到下一个动作的概率分布，通过价值网络计算可获胜的数值，并调整网络权重去逼近最终预测。



##### Thinking 3： 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的

蒙特卡洛算法概念：采样越多越近似最优解，在有限采样内尽可能准确的情况下可以采用这种思考模式。

前提：节点结构为 A/B结构，意义是某节点被访问了B次，正反馈（奖励）了A次（即走了B次这步棋，赢了A次）

Step 1 - Select：从根节点往下走，每次选择一个最有价值的子节点，知道找到“存在未扩展的子节点”（还没到末端、状态仍未结束）

Step 2 - Expansion：给这个子节点加上一个0/0节点

Step 3 - Simulation：用快速走子（Rollout policy）走到底，快速地得到一个胜负结果

Step 4 - Back propagation：把模拟结果加到所有的父节点上（如，正反馈就是给每个父节点加 1/1，负反馈就是0/1 即 (A+1)/(B+1) 或 A/(B+1)

重复做上述步骤，更新行动的价值得到对应的策略价值网络。



##### Thinking 4： 假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑

**基础思路**

针对推送给用户的每一个推流，制定一个正反馈的标准行为，如是否完整看完视频。用户完整看完视频的话，就给同类型的视频一个正反馈，然后给用户推荐同类的视频。



**扩展思路 **

**- 考虑并行的正负反馈价值网络**

正反馈的行为可以从一个更新为多个，如完整看完视频、看完90%的时间、或点赞，每完成一个行为就给当前视频以及当前视频的类型一个正反馈累加值，构成视频和视频分类的价值网络，向用户推荐分值较高的、点赞较多的同类视频

**- 考虑用户画像的推荐**

通过考虑拥有相同画像的UP主以及当前浏览的视频所属UP主，构成“观看者-内容生产者”之间的链接，从“只考虑观看者-视频”之间的关系，到考虑观看者-发布者-内容之间的关系，扩展价值网络的同时也考虑到了用户不仅仅喜欢视频、还喜欢发布者本身的想法



##### Thinking 5： 在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路

由于强化学习的概念是承接蒙特卡洛的思想，即尽可能采集更多的样本，所以需要多次尝试才能得到更优的结果，但实际情况下应该无法用真实场景进行尝试（成本太高）。

**前提：**强化学习的基础对象是有一定效果的预训练模型，以及需要规定一定的驾驶场景（如红灯停绿灯走，遇到障碍要减速等），减少学习成本。

**思路1：**通过虚拟驾驶的方式，每次触发驾驶交互行为的时候进行强化学习和累积当前场景下、对应行为的累积分数。该方法的好处是，电脑程序可以规定各种不同的场景进行**并行自动**学习再累加结果，也可以做连续场景。

**思路2：**通过各类传感器，以真人驾驶的方式开车，同时让模型自动学习驾驶行为。触发行为的方式有2种，一种是驾驶员先触发，一种是程序先触发。每次触发行为的时候记录“当前场景”下机器和驾驶员的行为差异并形成正负反馈（需要容忍一定的差异，如机器认为与前车剩余20米的时候要减速，但驾驶员是10米的时候才减速）。这种方式的好处是有点结合主动学习的意思（每个交互动作都有立即反馈），同时也可以多个驾驶员一起做测试，但成本十分高。