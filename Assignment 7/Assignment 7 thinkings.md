#### Thinking 1：在CTR点击率预估中，使用GBDT+LR的原理是什么？

1. GBDT+LR是通过stacking的方式改进了解决二分类问题的模型，用于预测用户CTR；
2. 现实中由于数据量庞大，经常采用计算速度极快的线性模型LR，但是单纯用LR的学习能力有限，而且特征工程十分重要，人力成本就会很高
3. GBDT+LR的结合方式可以通过树模型，自动发现有效的特征和特征组合，弥补人工经验不足的问题。同时，由于计算速度快，该模型在工业界也十分常用。



#### Thinking 2：Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）

Wdie&Deep模型：通过直接向LR模型输入稀疏onehot特征，提供wide的特征（该部分可解释强，同时具备记忆力）。Deep部分通过DNN模型学习低纬度稠密向量，使模型具有泛化能力，最后再输入到LR线性模式进行输出。



#### Thinking 3：在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？

1. FNN：通过FM进行参数初始化输入至wide&deep的deep模型
2. DeepFM：相当于把wide&deep的wide部分替换成FM模型，与Deep模型共享输入的并行结构
3. NFM：相当于DeepFM的串行版本。通过对嵌入层采用对位相乘后加起来作为交叉特征，然后通过DNN研所特征，最后合并线性和deep部分的特征。NFM可以解决特征交叉能力不足的问题。



#### Thinking 4：Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？

1. baseline算法原理是设置基线后，引入user和item的bias进行预测打分
2. KNNBaseline则是考虑到了每个用户的打分基线，先利用行为相似度计算用户相似度（找到和目标用户兴趣相似的用户集合），设置用户u对物品i的相似度，等价于K个邻域对物品i的兴趣度，最后生成推荐列表。而KNNBaseline是基于KNN_with_means的基础上，用baseline替代均值。



#### Thinking 5：GBDT和随机森林都是基于树的算法，它们有什么区别？

1. GBDT是一种boosting算法，每一棵树都会基于上一棵树的残差进行拟合和学习，消除残差。GBDT由多棵决策树组成，所有树的结论累加起来形成最终答案。
2. 随机森林采用的是bagging的思想，通过训练样本集中进行放回的采样得到多个采样机，针对每个采样训练一个学习器，再结合起来。同时决策树在训练过程中引入了随机属性选择。
3. 其它不同点：
   1. 随机森林可以是回归树，也可以是分类树，但是GBDT只能是回归树
   2. GBDT因为需要拟合上一棵树的残差，只能串行，随机森林可以并行
   3. 随机森林的结果是通过投票得到的，GBDT则是累加



#### Thinking 6：基于邻域的协同过滤都有哪些算法，请简述原理

1. UserCF：找到相似用户，推荐相似用户喜欢的物品给被推荐用户。原理：
   1. 找到和目标用户兴趣相似的用户集合
   2. 目标用户对物品的相似度，等价于K个邻居对物品的相似度
   3. 筛选出推荐列表
2. ItemCF：通过计算行为相似度，来计算用户的相似度。原理：
   1. 先计算物品之间的相似度
   2. 在针对目标用户对物品的兴趣程度，等价于物品的K个邻域物品，收到用户u的兴趣度
   3. 生成推荐列表